# -*- coding: utf-8 -*-
"""TechChallengeFase2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11LIXQX0FDM5hhhQYqwm1beIuSRd6tvC2
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import datetime as dt

from xgboost import XGBRegressor
from sklearn.model_selection import TimeSeriesSplit
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from statsmodels.stats.diagnostic import acorr_ljungbox

#Base de dados dos ultimos 10 anos
TabIbov = "https://raw.githubusercontent.com/Naiderossi/TechChallengeF2/main/Ibovespa3.csv"
#TabIbov = "/content/ibovespa3.csv"
df= pd.read_csv(TabIbov, sep=',' )
df.head()
df.shape

#Transformando a coluna Data em datetime
df['Data'] = pd.to_datetime(df['Data'], dayfirst=True)

#Realizando a ordenação do df
df.sort_values(by='Data', inplace=True, ascending=True)

#Colocando a a coluna Data como índice
df.set_index('Data', inplace=True)

df.head()

#Removendo simbolo percentual e transformando para decimal a coluna Var%
df['Var%'] = df['Var%'].str.replace(',', '.').str.replace('%', '')
df['Var%'] = pd.to_numeric(df['Var%'], errors='coerce') / 100

# Ajustar bilhões e milhões corretamente
df.replace({',': '.', 'K': 'e3','B': 'e9', 'M': 'e6'}, regex=True, inplace=True)  # Substituir símbolos

# Converter colunas numéricas para float (caso tenham sido transformadas em string)
df = df.apply(pd.to_numeric, errors='coerce')

df.head()

df.info()

#Verificando se existem nulos
linhas_nulas = df[df.isnull().any(axis=1)]
linhas_nulas

#Calcular a média do volume (ignorando valores nulos)
media_vol = df["Vol."].mean()

# Preencher os valores nulos com a mediana
df["Vol."]= df['Vol.'].fillna(media_vol)

df.isnull().sum()

df.head()

#Média móvel dos ultimos 21 dias (pregão)

df["Media_Movel_21d"] = df["Último"].rolling(window=21).mean()

#média geral do fechamento
media_geral = df["Último"].mean()

plt.figure(figsize=(20, 5))
plt.plot(df.index, df["Último"], label="Fechamento Diário", color="blue", alpha=0.5)
plt.plot(df.index, df["Media_Movel_21d"], label="Média Móvel 21 Dias", color="red", linewidth=2)
plt.axhline(y=media_geral, color="green", linestyle="dashed", linewidth=2, label="Média Geral")


plt.title("Média Móvel de 21 Dias e Média Geral do Fechamento")
plt.xlabel("Data")
plt.ylabel("Fechamento IBOVESPA")
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.show()

"""# Analisando variações por dia do mês e mês no ano"""

df['Dia'] = df.index.day
df['Semana'] = [dt.weekofyear for dt in df.index]
df['mes'] = df.index.month
df['ano'] = df.index.year

# Calcular a média e volatilidade da variação percentual por dia do mês
padroes_mes = df.groupby("Dia")["Último"].agg(["mean", "std"])

# Plotar os padrões mensais
plt.figure(figsize=(20, 5))
plt.plot(padroes_mes.index, padroes_mes["mean"], marker="o", linestyle="-", label="Média da Variação (%)", color="b")
plt.fill_between(padroes_mes.index,
                 padroes_mes["mean"] - padroes_mes["std"],
                 padroes_mes["mean"] + padroes_mes["std"],
                 color="b", alpha=0.2, label="Desvio Padrão")
plt.axvline(3, color="gray", linestyle="--", alpha=0.6, label="Início do Mês")
plt.axvline(15, color="red", linestyle="--", alpha=0.6, label="Meio do Mês")
plt.axvline(21, color="purple", linestyle="--", alpha=0.6, label="Semana de Vencimento de Opções")
plt.axvline(28, color="green", linestyle="--", alpha=0.6, label="Final do Mês")

plt.xlabel("Dia do Mês")
plt.ylabel("Média da Variação (%)")
plt.title("Padrões de Variação Percentual ao Longo do Mês")
plt.legend()
plt.grid()

# Exibir os dados estatísticos e gráfico
padroes_mes, plt.show()

# Calcular a média e volatilidade da variação percentual por dia do mês
year_patterns = df.groupby("mes")["Último"].agg(["mean", "std"])

plt.figure(figsize=(20, 5))
plt.bar(year_patterns.index, year_patterns["mean"], yerr=year_patterns["std"], capsize=5, color="blue", alpha=0.7)
plt.xlabel("Mês do Ano")
plt.ylabel("Média da Variação (%)")
plt.title("Padrões Sazonais do Ibovespa por Mês do Ano")
plt.xticks(range(1, 13), ["Jan", "Fev", "Mar", "Abr", "Mai", "Jun", "Jul", "Ago", "Set", "Out", "Nov", "Dez"])
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.show()

"""*  Padrão Mensal: A IBOVESPA tende a ser ligeiramente mais alta no final do mês e mais baixa no meio do mês.
*  Volatilidade: Alguns dias apresentam desvios padrão maiores, sugerindo que nesses períodos há mais incerteza no mercado.

# Analisando estacionariedade da variável target
"""

# Teste ADF
resultado_adf = adfuller(df["Último"].dropna())
print(f"Estatística ADF: {resultado_adf[0]}")
print(f"Valor-p: {100*resultado_adf[1]:.2f}%")


# Interpretação
if resultado_adf[1] < 0.05:
    print("A série é estacionária.")
else:
    print("A série não é estacionária.")

# Diferenciação de primeira ordem
df["Último_diff"] = df["Último"].diff().dropna()

# Plot da série diferenciada
plt.figure(figsize=(20, 5))
plt.plot(df.index, df["Último_diff"], label="Série Diferenciada (1ª Ordem)")
plt.title("Série Diferenciada (1ª Ordem)")
plt.xlabel("Data")
plt.ylabel("Diferença de Temperatura (°C)")
plt.legend()
plt.show()

resultado_adf_diff = adfuller(df["Último_diff"].dropna())
print(f"Estatística ADF: {resultado_adf_diff[0]}")
print(f"Valor-p: {100*resultado_adf_diff[1]:.4}%")

# Interpretação
if resultado_adf_diff[1] < 0.05:
    print("A série diferenciada é estacionária.")
else:
    print("A série diferenciada não é estacionária.")

# Plot da ACF e PACF da série diferenciada de 1 ordem
plt.figure(figsize=(20, 5))
plot_acf(df["Último_diff"].dropna(), ax=plt.gca(), lags=240)
plt.title("Função de Autocorrelação (ACF)")

plt.figure(figsize=(20, 5))
plot_pacf(df["Último_diff"].dropna(), ax=plt.gca(), lags=240)
plt.title("Função de Autocorrelação (PACF)")

"""

---


Os picos de autocorrelação no ACF e PACF parecem ocorrer em intervalos regulares, o que  pode ser um indicativo de sazonalidade. Nesse caso será necessário aplicar diferenciação sazonal, optando por um shift de 240 dias, pois é a média de dias úteis em um ano.


---

"""

# Diferenciação sazonal (período de 240 dias úteis)
df["Ultimo_seasonal_diff"] = df["Último"] - df["Último"].shift(240)
df["Ultimo_seasonal_diff"] = df["Ultimo_seasonal_diff"].dropna()

# Plot da série sazonalmente diferenciada
plt.figure(figsize=(20, 5))
plt.plot(
    df.index, df["Ultimo_seasonal_diff"], label="Série Diferenciada Sazonalmente"
)
plt.title("Série Diferenciada Sazonalmente")
plt.xlabel("Data")
plt.ylabel("Diferença de Temperatura (°C)")
plt.legend()

resultado_adf_seasonaldiff = adfuller(df["Ultimo_seasonal_diff"].dropna())
print(f"Estatística ADF: {resultado_adf_diff[0]}")
print(f"Valor-p: {100*resultado_adf_diff[1]:.4}%")

# Interpretação
if resultado_adf_diff[1] < 0.05:
    print("A série diferenciada é estacionária.")
else:
    print("A série diferenciada não é estacionária.")

# Plot da ACF e PACF da série diferenciada sazonalmente
plt.figure(figsize=(20, 5))
plot_acf(df["Ultimo_seasonal_diff"].dropna(), ax=plt.gca(), lags =240)
plt.title("Função de Autocorrelação (ACF)")

plt.figure(figsize=(20, 5))
plot_pacf(df["Ultimo_seasonal_diff"].dropna(), ax=plt.gca(), lags=240)
plt.title("Função de Autocorrelação (PACF)")

"""# Escolha dos modelos

Foram estudados diversos modelos de Machine Learning para previsão de séries temporaos. Para o trabalho em específico foi optado por aplicar os modelos de ML para previsão de séries temporais financeiras. Sendo eles:

  XGBoost:  um algoritmo baseado em árvores de decisão que utiliza gradient boosting para aprender padrões nos dados, sendo eficiente e robusto contra outliers. Ele é ideal para previsões de curto prazo, pois lida bem com dados não estacionários e grandes volumes de informação, mas exige engenharia de features para capturar sazonalidade e tendências de longo prazo, o que pode ser desafiador.

   LSTM (Long Short-Term Memory): são especializadas em aprender padrões temporais, tornando-se uma excelente opção para previsões de médio e longo prazo. Elas conseguem capturar dependências complexas e padrões sazonais automaticamente, o que as torna valiosas para análises financeiras. No entanto, possuem um alto custo computacional, necessitam de grandes quantidades de dados e são menos interpretáveis do que modelos estatísticos.

   SARIMAX (Seasonal AutoRegressive Integrated Moving Average with Exogenous Variables) é um modelo estatístico que combina autoregressão, médias móveis e diferenciação para capturar padrões sazonais e tendências. Ele é amplamente utilizado para séries temporais com padrões bem definidos e oferece alta interpretabilidade. No entanto, tem dificuldades para lidar com alta volatilidade, e exige que os dados sejam estacionários para melhor desempenho.

  Tendo em vista que a escolha do modelo ideal depende do tipo de dados e do horizonte da previsão, e o caso analisado busca curto prazo e possui alta volatilidade, será aplicado  XGBoost. Para sanar os  padrões sazonais da série (como o mês de pregão ou anos como de pandemia), o SARIMAX se utilizado. Já para suprir os padrões complexos de série fiananceiras, será testado o LSTM.

# Aplicando o modelo SARIMAX na série diferenciada sazonalmente
"""

#Definindo os parâmetros do modelo
p = 1 # 3 é ponto em que o gráfico PACF cruza a área azul pela primeira vez, porém foi reduzido devido ao tempo elevado de processamento do modelo e baixo desempenho
q = 1 # reduzido de 18 para 1 para melhorar a performance de processamento do modelo
d = 1

#Separando dados de treinamento e dados de teste
train_start_Sarimax = '2022-12-31'
train_end_Sarimax = '2025-01-20'
df_train_Sarimax = df.loc[train_start_Sarimax:train_end_Sarimax] #512 dias para treinamento
df_test_Sarimax = df.loc[train_end_Sarimax:] # 10 dias para teste

"""Após vários testes com tamanhos de base de treino e teste, o melhor desempenho do SARIMAX se deu a partir de um período médio de 2 anos para treino e 10 dias para testes."""

df_train_Sarimax.shape

df_test_Sarimax.shape

df_train_Sarimax = df_train_Sarimax[df_train_Sarimax['Ultimo_seasonal_diff'] < df_train_Sarimax['Ultimo_seasonal_diff'].quantile(0.95)]

sarimax = SARIMAX(df_train_Sarimax['Ultimo_seasonal_diff'], seasonal_order=(p, d, q, 21))

sarimax_fit = sarimax.fit(disp = False)

# Predict on test set
sarimax_pred = sarimax_fit.get_forecast(steps=len(df_test_Sarimax)).predicted_mean
mse_sarimax = mean_squared_error(df_test_Sarimax['Ultimo_seasonal_diff'], sarimax_pred)
mae_sarimax = mean_absolute_error(df_test_Sarimax['Ultimo_seasonal_diff'], sarimax_pred)
rmse_sarimax = mean_squared_error(df_test_Sarimax['Ultimo_seasonal_diff'], sarimax_pred)
mape_sarimax = 100 * mean_absolute_percentage_error(df_test_Sarimax['Ultimo_seasonal_diff'], sarimax_pred)

"""**Resultados do desempenho SARIMAX**"""

print(f'SARIMAX MAE: {mae_sarimax}')
print(f'SARIMAX MSE: {mse_sarimax}')
print(f'SARIMAX RMSE: {rmse_sarimax}')
print(f'SARIMAX MAPE: {mape_sarimax}')
print(f'Accuracy (%): {100 - mape_sarimax}')

# Definir intervalo de confiança com base no erro absoluto médio (MAE)
erro_max = sarimax_pred + mae_sarimax
erro_min = sarimax_pred - mae_sarimax

plt.figure(figsize=(20, 5))

# Plotar valores reais
plt.plot(df_test_Sarimax.index, df_test_Sarimax["Ultimo_seasonal_diff"], label="Real", color="blue", linewidth=2)

# Plotar previsões do modelo SARIMAX
plt.plot(df_test_Sarimax.index, sarimax_pred, label="Predição SARIMAX", color="red", linestyle="dashed", linewidth=2)

# Plotar banda de erro máximo e mínimo
plt.fill_between(df_test_Sarimax.index, erro_min, erro_max, color="pink", alpha=0.3, label="Intervalo de Erro (±MAE)")


plt.title("Previsão SARIMAX vs. Valores Reais com Intervalo de Erro")
plt.xlabel("Data")
plt.ylabel("Fechamento (Diferença Sazonal)")
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.show()

"""# Aplicando o modelo XGboost

1. Criar medidas predição para treinar o modelo
"""

#Definição de 21 dias para a janela de lags (Período médio de um mês útil)
n_lags = 21
for lag in range(1, n_lags + 1):
    df[f'lag_{lag}'] = df['Último'].shift(lag)

# Média de janela móvel
df[f'rolling_mean_{n_lags}'] = df['Último'].rolling(window=n_lags).mean().shift(1)
df[f'rolling_std_{n_lags}'] = df['Último'].rolling(window=n_lags).std().shift(1)

# Média móvel exponencial  (EMA)
df[f'ema_{n_lags}'] = df['Último'].ewm(span=n_lags).mean().shift(1)
df[f'ema_std_{n_lags}'] = df['Último'].ewm(span=n_lags).std().shift(1)

# Medidas técnicas
df["rsi_14"] = 100 - (100 / (1 + df["Último"].pct_change().rolling(14).mean()))  # RSI simples
df["macd"] = df["Último"].ewm(span=12, adjust=False).mean() - df["Último"].ewm(span=26, adjust=False).mean()

import warnings
from scipy.signal import periodogram

warnings.filterwarnings("ignore")

frequency, power = periodogram(df['Último'].dropna(), fs=1)
fig, ax = plt.subplots(figsize=(20, 5))
ax.plot(frequency[1:], power[1:])
ax.set_title('Power Spectrum')
ax.set_xlabel('Frequency')
ax.set_ylabel('Power')
ax.set_yscale('log')
ax.get_figure().set_tight_layout(True)

freq = frequency[np.argmax(power)]
period = 1 / freq

print(f'Period: {period} days')

df['fourier_sin'] = np.sin(2 * np.pi * freq * np.arange(len(df)))
df['fourier_cos'] = np.cos(2 * np.pi * freq * np.arange(len(df)))

df[['fourier_sin', 'fourier_cos']].plot(subplots=True)

#Separando dados de treino e teste para o modelo XGboost
train_start = '2010-01-01'
train_end = '2025-01-15'
df_train = df.loc[train_start:train_end]
df_test = df.loc[train_end:]

df_test.tail()

predictors = [
    'Dia',
    'Semana',
    'mes',
    'ano',
    *['lag_{}'.format(i) for i in range(1, n_lags + 1)],
    'rolling_mean_{n_lags}'.format(n_lags=n_lags),
    'rolling_std_{n_lags}'.format(n_lags=n_lags),
    'ema_{n_lags}'.format(n_lags=n_lags),
    'ema_std_{n_lags}'.format(n_lags=n_lags),
    "rsi_14",
    "macd",
    'fourier_sin',
    'fourier_cos',
]

df_train.columns

X_train = df_train[predictors]
y_train = df_train['Último']
X_test = df_test[predictors]
y_test = df_test['Último']

import xgboost as xgb

# TimeSeriesSplit for validation
tscv = TimeSeriesSplit(n_splits=20)
xgb_models = []
val_mses = []

params = {
    "objective": "reg:squarederror",
    "n_estimators": 500,
    "learning_rate": 0.05,
    "max_depth": 5,
    "subsample": 0.7,
    "colsample_bytree": 0.5,
    "gamma": 0.3,
    "min_child_weight": 8,
    "random_state": 42,
    "eval_metric": "rmse"
}

# Train with cross-validation
for train_idx, val_idx in tscv.split(X_train):
    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

 # Criando a estrutura DMatrix do XGBoost
    dtrain = xgb.DMatrix(X_tr, label=y_tr)
    dval = xgb.DMatrix(X_val, label=y_val)

   # Treinamento do modelo com early stopping
    evals = [(dtrain, "train"), (dval, "eval")]
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=500,
        evals=evals,
        early_stopping_rounds=30,
        verbose_eval=False
    )

# Validação e erro médio
    val_pred = model.predict(dval)
    mse = mean_squared_error(y_val, val_pred)
    val_mses.append(mse)
    print(f'Validation MSE: {mse:.2f}')

    # Armazena o modelo treinado
    xgb_models.append(model)

# Exibir importância das features do último modelo treinado
importances = xgb_models[-1].get_score(importance_type="weight")
feature_importance = pd.DataFrame({"Feature": importances.keys(), "Importance": importances.values()})
feature_importance = feature_importance.sort_values(by="Importance", ascending=False)
print("\nFeature Importance:\n", feature_importance)

# Previsões na base de teste
dtest = xgb.DMatrix(X_test)
xgb_pred = np.mean([model.predict(dtest) for model in xgb_models], axis=0)

"""**Resultados do desempenho XGBoost**"""

mse_xgb = mean_squared_error(y_test, xgb_pred)
mae_xgb = mean_absolute_error(y_test, xgb_pred)
rmse_xgb = mean_squared_error(y_test, xgb_pred)
mape_xgb = 100 * mean_absolute_percentage_error(y_test, xgb_pred)
print(f'XGBoost MSE: {mse_xgb}')
print(f'XGBoost MAE: {mae_xgb}')
print(f'XGBoost RMSE: {rmse_xgb}')
print(f'XGBoost MAPE: {mape_xgb}')
print(f'Accuracy (%): {100 - mape_xgb}')

"""Removendo variáveis de predição com baixa importância"""

predictors2 = [
    'Dia',
    'Semana',
    *['lag_{}'.format(i) for i in range(1, n_lags + 1)],
    #'rolling_mean_{n_lags}'.format(n_lags=n_lags),
    #'rolling_std_{n_lags}'.format(n_lags=n_lags),
    #'ema_{n_lags}'.format(n_lags=n_lags),
    'ema_std_{n_lags}'.format(n_lags=n_lags),
    "rsi_14",
    "macd",
    'fourier_sin',
    'fourier_cos',
]

X_train2 = df_train[predictors2]
y_train2 = df_train['Último']
X_test2 = df_test[predictors2]
y_test2 = df_test['Último']

# TimeSeriesSplit for validation
tscv2 = TimeSeriesSplit(n_splits=20)
xgb_models2 = []
val_mses2 = []

params2 = {
    "objective": "reg:squarederror",
    "n_estimators": 500,
    "learning_rate": 0.05,
    "max_depth": 5,
    "subsample": 0.7,
    "colsample_bytree": 0.5,
    "gamma": 0.3,
    "min_child_weight": 8,
    "random_state": 42,
    "eval_metric": "rmse"
}

# Train with cross-validation
for train_idx2, val_idx2 in tscv2.split(X_train2):
    X_tr2, X_val2 = X_train2.iloc[train_idx2], X_train2.iloc[val_idx2]
    y_tr2, y_val2 = y_train2.iloc[train_idx2], y_train2.iloc[val_idx2]

 # Criando a estrutura DMatrix do XGBoost
    dtrain2 = xgb.DMatrix(X_tr2, label=y_tr2)
    dval2 = xgb.DMatrix(X_val2, label=y_val2)

   # Treinamento do modelo com early stopping
    evals = [(dtrain2, "train"), (dval2, "eval")]
    model = xgb.train(
        params2,
        dtrain2,
        num_boost_round=500,
        evals=evals,
        early_stopping_rounds=30,
        verbose_eval=False
    )

# Validação e erro médio
    val_pred2 = model.predict(dval2)
    mse2 = mean_squared_error(y_val2, val_pred2)
    val_mses2.append(mse2)
    print(f'Validation MSE: {mse2:.2f}')

    # Armazena o modelo treinado
    xgb_models2.append(model)

# Exibir importância das features do último modelo treinado
importances = xgb_models2[-1].get_score(importance_type="weight")
feature_importance = pd.DataFrame({"Feature": importances.keys(), "Importance": importances.values()})
feature_importance = feature_importance.sort_values(by="Importance", ascending=False)
print("\nFeature Importance:\n", feature_importance)

# Previsões na base de teste
dtest2 = xgb.DMatrix(X_test2)
xgb_pred2 = np.mean([model.predict(dtest2) for model in xgb_models2], axis=0)

"""**Resultados do desempenho do modelo com menos preditores **"""

mse_xgb2 = mean_squared_error(y_test2, xgb_pred2)
mae_xgb2 = mean_absolute_error(y_test2, xgb_pred2)
rmse_xgb2 = mean_squared_error(y_test2, xgb_pred2)
mape_xgb2 = 100 * mean_absolute_percentage_error(y_test2, xgb_pred2)
print(f'XGBoost MSE: {mse_xgb2}')
print(f'XGBoost MAE: {mae_xgb2}')
print(f'XGBoost RMSE: {rmse_xgb2}')
print(f'XGBoost MAPE: {mape_xgb2}')
print(f'Accuracy (%): {100 - mape_xgb2}')

# Definir intervalo de confiança com base nos erros
upper_bound = xgb_pred + mae_xgb
lower_bound = xgb_pred - mae_xgb

# Criar gráfico
plt.figure(figsize=(20, 5))

# Plotar valores reais
plt.plot(X_test.index, y_test, label="Real", color="blue", linewidth=2)

# Plotar previsões do modelo XGBoost
plt.plot(X_test.index, xgb_pred, label="Predição (XGBoost)", color="red", linestyle="dashed", linewidth=2)

# Plotar banda de intervalo de confiança usando RMSE
plt.fill_between(X_test.index, lower_bound, upper_bound, color="pink", alpha=0.3, label="Intervalo de Confiança")

# Configurar título e legendas
plt.xlabel("Data")
plt.ylabel("Fechamento IBOVESPA")
plt.title("Predição do Fechamento Diário da IBOVESPA com Intervalo de Confiança")
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)

# Exibir gráfico
plt.show()

"""# Aplicando o modelo LSTM"""

import tensorflow as tf
from tf_keras.models import Sequential
from tf_keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tf_keras.callbacks import EarlyStopping
from tf_keras.regularizers import l2

# Normalizando os dados
scaler = MinMaxScaler(feature_range=(0, 1))
train_scaled = scaler.fit_transform(df_train[['Último']].values.reshape(-1, 1))
test_scaled = scaler.transform(df_test[['Último']].values.reshape(-1, 1))

# Criando sequência de entrada para o LSTM
def create_sequences(data, sequence_length):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data[i:i + sequence_length])
        y.append(data[i + sequence_length])
    return np.array(X), np.array(y)

sequence_length = 5
X_train, y_train = create_sequences(train_scaled, sequence_length)
X_test, y_test = create_sequences(test_scaled, sequence_length)

# Ajustando para 3D (necessário para o LSTM)
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Construindo o modelo LSTM otimizado
lstm = Sequential([
    LSTM(50, activation='tanh', return_sequences=True, input_shape=(sequence_length, 1),
         kernel_regularizer=l2(0.01)),  # Regularização L2
    Dropout(0.5),  # Dropout mais alto para evitar overfitting

    LSTM(25, activation='tanh', return_sequences=False),
    Dropout(0.5),


    Dense(1)
])

# Compilando o modelo com Adam otimizado
lstm.compile(optimizer='adam', loss='mse')

# Early Stopping para evitar overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Treinando o modelo com validação
lstm.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_data=(X_test, y_test),
         callbacks=[early_stopping])

# Fazendo previsões
lstm_pred = lstm.predict(X_test)

# Revertendo a escala das previsões
lstm_pred = scaler.inverse_transform(lstm_pred.reshape(-1, 1))
y_test = scaler.inverse_transform(y_test.reshape(-1, 1))

"""**Resultados do desempenho LSTM**"""

# Calculando o MSE final
mse_lstm = mean_squared_error(y_test, lstm_pred)
mae_lstm = mean_absolute_error(y_test, lstm_pred)
rmse_lstm = mean_squared_error(y_test, lstm_pred)
mape_lstm = 100 * mean_absolute_percentage_error(y_test, lstm_pred)
print(f'LSTM MSE: {mse_lstm}')
print(f'LSTM MAE: {mae_lstm}')
print(f'LSTM RMSE: {rmse_lstm}')
print(f'LSTM MAPE: {mape_lstm}')
print(f'Accuracy (%): {100 - mape_lstm}')

df_results = pd.DataFrame({
    'Data': df_test.index[-len(y_test):],
    'Real': y_test.flatten(),
    'Previsto': lstm_pred.flatten()
})

# Calculando intervalo de confiança (supondo um desvio padrão estimado)
desvio_padrao = np.std(y_test - lstm_pred)
df_results['Previsto_max'] = df_results['Previsto'] + 1.96 * desvio_padrao
df_results['Previsto_min'] = df_results['Previsto'] - 1.96 * desvio_padrao

# Criando o gráfico
plt.figure(figsize=(20, 5))
sns.lineplot(data=df_results, x='Data', y='Real', label='Valor Real', color='blue', linewidth=2)
sns.lineplot(data=df_results, x='Data', y='Previsto', label='Previsão LSTM', color='red', linestyle='dashed', linewidth=2)

# Adicionando a faixa do intervalo de confiança
plt.fill_between(df_results['Data'], df_results['Previsto_min'], df_results['Previsto_max'],
                 color='red', alpha=0.2, label='Intervalo de Confiança')

# Configurações do gráfico
plt.xlabel('Data')
plt.ylabel('Valor do Índice')
plt.title('Comparação entre Valor Real e Previsão do LSTM')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.show()

"""# Conclusão"""

from tabulate import tabulate
from IPython.display import display

# Criando um DataFrame com os resultados dos modelos
df_comparacao = pd.DataFrame({
    "Modelo": ["SARIMAX", "XGBoost", "LSTM"],
    "MSE": [1.41, 732.88, 2.47],
    "MAE": [3.58, 27.04, 1.33],
    "RMSE": [3.58, 732.88, 2.47],
    "MAPE (%)": [34.10, 21.89, 1.09],
    "Accuracy (%)": [65.89, 78.10, 99.10]
})

# Exibir a tabela no formato tabular
display(df_comparacao)

"""* O modelo LSTM foi o mais eficaz na previsão do fechamento diário do IBOVESPA, apresentando a menor margem de erro e maior precisão. Também é possível que haja um overfitting no modelo, visto a acurácia alta, mas talvez com uma base de dados que supere os 20 anos tenha uma performance real de aprendizagem melhor.

* O XGBoost pode ser aprimorado para melhor desempenho, mas seu erro absoluto ainda é muito alto. Provavelmente há falha nos dados de predição. Mesmo retirando alguns dados de predição com menor importância, o modelo permaneceu apresentando um mse bem alto, porém com uma acurácia bastante aceitável.

* O SARIMAX é o menos eficaz, mostrando alta variação e menor precisão, mesmo utilizando os dados diferenciados sazonalmente e tendo reduzido a base de aprendizado. Como imaginado, a volatilidade dos dados prejudicou o aprendizado do modelo.



"""